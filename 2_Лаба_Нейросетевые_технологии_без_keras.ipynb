{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Whwgm-R-C5ciFr37AyojKQSDww88NmH1",
      "authorship_tag": "ABX9TyPnUReChdve+nEdSthJepvZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MorozovDesu/neural_network_technologies/blob/main/2_%D0%9B%D0%B0%D0%B1%D0%B0_%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D1%81%D0%B5%D1%82%D0%B5%D0%B2%D1%8B%D0%B5_%D1%82%D0%B5%D1%85%D0%BD%D0%BE%D0%BB%D0%BE%D0%B3%D0%B8%D0%B8_%D0%B1%D0%B5%D0%B7_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random # библиотека функций для генерации случайных значений\n",
        "# Сторонние библиотеки\n",
        "import numpy as np # библиотека функций для работы с матрицами\n",
        "\"\"\" ---Раздел описаний--- \"\"\"\n",
        "\"\"\" --Описание класса Network--\"\"\"\n",
        "class Network(object): # используется для описания нейронной сети\n",
        "    def __init__(self, sizes): # конструктор класса\n",
        "# self – указатель на объект класса\n",
        "# sizes – список размеров слоев нейронной сети\n",
        "        self.num_layers = len(sizes) # задаем количество слоев нейронной сети\n",
        "        self.sizes = sizes # задаем список размеров слоев нейронной сети\n",
        "        self.biases = [np.random.randn(y, 1)*math.sqrt(2/y) for y in sizes[1:]] # задаем случайные начальные смещения\n",
        "        self.weights = [np.random.randn(y, x)*math.sqrt(2/y) for x, y in zip(sizes[:-1], sizes[1:])] # задаем случайные начальные веса связей\n",
        "    def sigmoid(self,z): # определение сигмоидальной функции активации\n",
        "        return np.sinh(z)/np.cosh(z)\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = self.sigmoid(np.dot(w, a)+b)\n",
        "        return a\n",
        "    def SGD( # Стохастический градиентный спуск\n",
        "        self # указатель на объект класса\n",
        "        , training_data # обучающая выборка\n",
        "        , epochs # количество эпох обучения\n",
        "        , mini_batch_size # размер подвыборки\n",
        "        , eta # скорость обучения\n",
        "        , test_data # тестирующая выборка\n",
        "        ):\n",
        "        test_data = list(test_data) # создаем список объектов тестирующей выборки\n",
        "        n_test = len(test_data) # вычисляем длину тестирующей выборки\n",
        "        training_data = list(training_data) # создаем список объектов обучающей выборки\n",
        "        n = len(training_data) # вычисляем размер обучающей выборки\n",
        "        for j in range(epochs): # цикл по эпохам\n",
        "            random.shuffle(training_data) # перемешиваем элементы обучающей выборки\n",
        "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)] # создаем подвыборки\n",
        "            for mini_batch in mini_batches: # цикл по подвыборкам\n",
        "              #print(len(mini_batch[0][0]))\n",
        "              self.update_mini_batch(mini_batch, eta) # один шаг градиентного спуска\n",
        "            print (\"Epoch {0}: {1} / {2}\".format(j, self.evaluate(test_data), n_test)) # смотрим прогресс в обучении\n",
        "    def update_mini_batch( # Шаг градиентного спуска\n",
        "        self # указатель на объект класса\n",
        "        , mini_batch # подвыборка\n",
        "        , eta # скорость обучения\n",
        "        ):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases] # список градиентов dC/db для каждого слоя (первоначально заполняются нулями)\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights] # список градиентов dC/dw для каждого слоя (первоначально заполняются нулями)\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y) # послойно вычисляем градиенты dC/db и dC/dw для текущего прецедента (x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] # суммируем градиенты dC/db для различных прецедентов текущей подвыборки\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] # суммируем градиенты dC/dw для различных прецедентов текущей подвыборки\n",
        "        self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] # обновляем все веса w нейронной сети\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] # обновляем все смещения b нейронной сети\n",
        "    def backprop( # Алгоритм обратного распространения\n",
        "        self # указатель на объект класса\n",
        "      ,x # вектор входных сигналов ,\n",
        "      ,y # ожидаемый вектор выходных сигналов\n",
        "      ):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases] # список градиентов dC/db для каждого слоя (первоначально заполняются нулями)\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights] # список градиентов dC/dw для каждого слоя (первоначально заполняются нулями)\n",
        "        # определение переменных\n",
        "        activation = x # выходные сигналы слоя (первоначально соответствует выходным сигналам 1-го слоя или входным сигналам сети)\n",
        "        activations = [x] # список выходных сигналов по всем слоям (первоначально содержит только выходные сигналы 1-го слоя)\n",
        "        zs = [] # список активационных потенциалов по всем слоям (первоначально пуст)\n",
        "        # прямое распространение\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation)+b # считаем активационные потенциалы текущего слоя\n",
        "            zs.append(z) # добавляем элемент (активационные потенциалы слоя) в конец списка\n",
        "            activation = self.sigmoid(z) # считаем выходные сигналы текущего слоя, применяя сигмоидальную функцию активации к активационным потенциалам слоя\n",
        "            activations.append(activation) # добавляем элемент (выходные сигналы слоя) в конец списка\n",
        "  # обратное распространение\n",
        "        delta = self.cost_derivative(activations[-1], y) * self.sigmoid_prime(zs[-1]) # считаем меру влияния нейронов выходного слоя L на величину ошибки (BP1)\n",
        "        nabla_b[-1] = delta # градиент dC/db для слоя L (BP3)\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # градиент dC/dw для слоя L (BP4)\n",
        "        for l in range(2, self.num_layers):\n",
        "          z = zs[-l] # активационные потенциалы l-го слоя (двигаемся по списку справа налево)\n",
        "          sp = self.sigmoid_prime(z) # считаем сигмоидальную функцию от активационных потенциалов l-го слоя\n",
        "          delta = np.dot(self.weights[-l+1].transpose(), delta) * sp # считаем меру влияния нейронов l-го слоя на величину ошибки (BP2)\n",
        "          nabla_b[-l] = delta # градиент dC/db для l-го слоя (BP3)\n",
        "          nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())# градиент dC/dw для l-го слоя (BP4)\n",
        "        return (nabla_b, nabla_w)\n",
        "    def evaluate(self, test_data): # Оценка прогресса в обучении\n",
        "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
        "        return sum(int(x == y) for (x, y) in test_results)\n",
        "    def cost_derivative(self, output_activations, y): # Вычисление частных производных стоимостной функции по выходным сигналам последнего слоя\n",
        "      return (output_activations-y)\n",
        "    def sigmoid_prime(self,z):# Производная сигмоидальной функции\n",
        "      return 1/np.cosh(z)**2\n"
      ],
      "metadata": {
        "id": "Ypitok4qrqTt"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}